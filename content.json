{"meta":{"title":"小骨子还在努力","subtitle":"还在努力","description":"","author":"xiao hong zhong","url":"http://xiaohongzhong.top","root":"/"},"pages":[{"title":"about","date":"2020-11-18T06:59:25.000Z","updated":"2020-11-19T01:30:32.000Z","comments":true,"path":"about/index.html","permalink":"http://xiaohongzhong.top/about/index.html","excerpt":"","text":"我是小骨子，欢迎您"},{"title":"contact","date":"2020-11-22T11:54:42.000Z","updated":"2020-11-22T11:55:46.000Z","comments":true,"path":"contact/index.html","permalink":"http://xiaohongzhong.top/contact/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-11-18T06:58:27.000Z","updated":"2020-11-18T08:09:12.000Z","comments":true,"path":"tags/index.html","permalink":"http://xiaohongzhong.top/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-11-18T06:58:54.000Z","updated":"2020-11-18T08:00:20.000Z","comments":true,"path":"categories/index.html","permalink":"http://xiaohongzhong.top/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"网络VGG的简单记录","slug":"网络VGG的简单记录","date":"2020-12-10T01:51:06.000Z","updated":"2020-12-10T03:43:56.921Z","comments":true,"path":"2020/12/10/wang-luo-vgg-de-jian-dan-ji-lu/","link":"","permalink":"http://xiaohongzhong.top/2020/12/10/wang-luo-vgg-de-jian-dan-ji-lu/","excerpt":"","text":"VGG 以第一处卷积为例VGG19 卷积层默认的stride是(1, 1)填充方式是same，说明输入的大小和输出的大小一样，都是224卷积核越大，计算量暴增，不利于模型深度的增加，计算性能也会降低。以第一处池化为例MaxPooling2D的padding默认是valid输出大小= (224 - (pool_size=2) + 2 * (p=0)) / (stride=2) + 1 = (224 - 2 + 0)/2 + 1 = 112 keras下的vgg19代码 # -*- coding: utf-8 -*- &quot;&quot;&quot;VGG19 model for Keras. # Reference - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) &quot;&quot;&quot; from __future__ import absolute_import from __future__ import division from __future__ import print_function import os import warnings from ..models import Model from ..layers import Flatten from ..layers import Dense from ..layers import Input from ..layers import Conv2D from ..layers import MaxPooling2D from ..layers import GlobalAveragePooling2D from ..layers import GlobalMaxPooling2D from ..engine.topology import get_source_inputs from ..utils import layer_utils from ..utils.data_utils import get_file from .. import backend as K from .imagenet_utils import decode_predictions from .imagenet_utils import preprocess_input from .imagenet_utils import _obtain_input_shape WEIGHTS_PATH = &#39;https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5&#39; WEIGHTS_PATH_NO_TOP = &#39;https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5&#39; def VGG19(include_top=True, weights=&#39;imagenet&#39;, input_tensor=None, input_shape=None, pooling=None, classes=1000): if not (weights in &#123;&#39;imagenet&#39;, None&#125; or os.path.exists(weights)): raise ValueError(&#39;The `weights` argument should be either &#39; &#39;`None` (random initialization), `imagenet` &#39; &#39;(pre-training on ImageNet), &#39; &#39;or the path to the weights file to be loaded.&#39;) if weights == &#39;imagenet&#39; and include_top and classes != 1000: raise ValueError(&#39;If using `weights` as imagenet with `include_top`&#39; &#39; as true, `classes` should be 1000&#39;) # Determine proper input shape input_shape = _obtain_input_shape(input_shape, default_size=224, min_size=48, data_format=K.image_data_format(), require_flatten=include_top, weights=weights) if input_tensor is None: img_input = Input(shape=input_shape) else: if not K.is_keras_tensor(input_tensor): img_input = Input(tensor=input_tensor, shape=input_shape) else: img_input = input_tensor # Block 1 x = Conv2D(64, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block1_conv1&#39;)(img_input) x = Conv2D(64, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block1_conv2&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block1_pool&#39;)(x) # Block 2 x = Conv2D(128, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block2_conv1&#39;)(x) x = Conv2D(128, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block2_conv2&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block2_pool&#39;)(x) # Block 3 x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv1&#39;)(x) x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv2&#39;)(x) x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv3&#39;)(x) x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv4&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block3_pool&#39;)(x) # Block 4 x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv1&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv2&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv3&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv4&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block4_pool&#39;)(x) # Block 5 x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv1&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv2&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv3&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv4&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block5_pool&#39;)(x) if include_top: # Classification block x = Flatten(name=&#39;flatten&#39;)(x) x = Dense(4096, activation=&#39;relu&#39;, name=&#39;fc1&#39;)(x) x = Dense(4096, activation=&#39;relu&#39;, name=&#39;fc2&#39;)(x) x = Dense(classes, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x) else: if pooling == &#39;avg&#39;: x = GlobalAveragePooling2D()(x) elif pooling == &#39;max&#39;: x = GlobalMaxPooling2D()(x) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None: inputs = get_source_inputs(input_tensor) else: inputs = img_input # Create model. model = Model(inputs, x, name=&#39;vgg19&#39;) # load weights if weights == &#39;imagenet&#39;: if include_top: weights_path = get_file(&#39;vgg19_weights_tf_dim_ordering_tf_kernels.h5&#39;, WEIGHTS_PATH, cache_subdir=&#39;models&#39;, file_hash=&#39;cbe5617147190e668d6c5d5026f83318&#39;) else: weights_path = get_file(&#39;vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;, WEIGHTS_PATH_NO_TOP, cache_subdir=&#39;models&#39;, file_hash=&#39;253f8cb515780f3b799900260a226db6&#39;) model.load_weights(weights_path) if K.backend() == &#39;theano&#39;: layer_utils.convert_all_kernels_in_model(model) if K.image_data_format() == &#39;channels_first&#39;: if include_top: maxpool = model.get_layer(name=&#39;block5_pool&#39;) shape = maxpool.output_shape[1:] dense = model.get_layer(name=&#39;fc1&#39;) layer_utils.convert_dense_weights_data_format(dense, shape, &#39;channels_first&#39;) if K.backend() == &#39;tensorflow&#39;: warnings.warn(&#39;You are using the TensorFlow backend, yet you &#39; &#39;are using the Theano &#39; &#39;image data format convention &#39; &#39;(`image_data_format=&quot;channels_first&quot;`). &#39; &#39;For best performance, set &#39; &#39;`image_data_format=&quot;channels_last&quot;` in &#39; &#39;your Keras config &#39; &#39;at ~/.keras/keras.json.&#39;) elif weights is not None: model.load_weights(weights) return model 3x3是最小的能够捕获像素八邻域信息的尺寸。 两个3x3的堆叠卷基层的有限感受野是5x5；三个3x3的堆叠卷基层的感受野是7x7，故可以通过小尺寸卷积层的堆叠替代大尺寸卷积层，并且感受野大小不变。 多个3x3的卷基层比一个大尺寸filter卷基层有更多的非线性（更多层的非线性函数），使得判决函数更加具有判决性。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"经典神经网络","slug":"经典神经网络","permalink":"http://xiaohongzhong.top/tags/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"author":"xiao"},{"title":"图像分类-自己的数据集-Pytorch","slug":"图像分类-自己的数据集-Pytorch","date":"2020-12-08T09:57:29.000Z","updated":"2020-12-09T03:26:33.903Z","comments":true,"path":"2020/12/08/tu-xiang-fen-lei-zi-ji-de-shu-ju-ji-pytorch/","link":"","permalink":"http://xiaohongzhong.top/2020/12/08/tu-xiang-fen-lei-zi-ji-de-shu-ju-ji-pytorch/","excerpt":"","text":"数据集整理划分train_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_datasets = datasets.ImageFolder(&#39;dataset/logos_v2&#39;, transform=train_transforms) train_size = int(0.8 * len(train_datasets)) test_size = len(train_datasets) - int(0.8 * len(train_datasets)) train_dataset, test_dataset = torch.utils.data.random_split(train_datasets, [train_size, test_size]) train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0) test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0) 训练可视化一开始先定义 from visdom import Visdom # 将窗口类实例化 viz = Visdom() viz.line([[0., 0.]], [0], win=&#39;train&#39;, opts=dict(title=&#39;train&#39;, legend=[&#39;train_loss&#39;, &#39;train_acc&#39;])) viz.line([[0., 0.]], [0], win=&#39;test&#39;, opts=dict(title=&#39;test&#39;, legend=[&#39;test_loss&#39;, &#39;test_acc&#39;])) 后面再记录loss和acc的变化 # 更新窗口图像 viz.line([[running_loss / len(train_dataset), train_acc / len(train_dataset)]], [epoch], win=&#39;train&#39;, update=&#39;append&#39;) model.train()和model.eval()eval()时，pytorch会自动把BN和DropOut固定住，不会取平均，而是用训练好的值。不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大。 完整代码import torch from torchvision import models, datasets, transforms import matplotlib.pyplot as plt import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from visdom import Visdom # 将窗口类实例化 viz = Visdom() viz.line([[0., 0.]], [0], win=&#39;train&#39;, opts=dict(title=&#39;train&#39;, legend=[&#39;train_loss&#39;, &#39;train_acc&#39;])) viz.line([[0., 0.]], [0], win=&#39;test&#39;, opts=dict(title=&#39;test&#39;, legend=[&#39;test_loss&#39;, &#39;test_acc&#39;])) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) train_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_datasets = datasets.ImageFolder(&#39;dataset/logos_v2&#39;, transform=train_transforms) train_size = int(0.8 * len(train_datasets)) test_size = len(train_datasets) - int(0.8 * len(train_datasets)) train_dataset, test_dataset = torch.utils.data.random_split(train_datasets, [train_size, test_size]) train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0) test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0) classes = (&#39;bear&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;chicken&#39;, &#39;coffee&#39;, &#39;dog&#39;, &#39;duck&#39;, &#39;fish&#39;, &#39;flower&#39;, &#39;tree&#39;) net = models.vgg19(pretrained=True) net.to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 训练网络 for epoch in range(10): net.train() running_loss = 0.0 train_acc = 0.0 for i, data in enumerate(train_dataloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() # 经过测试，pred1 = torch.max(outputs, 1)[1]是和_, pred = torch.max(outputs.data, 1) 的pred相等的！！！ #pred1 = torch.max(outputs, 1)[1] _, pred = torch.max(outputs.data, 1) train_correct = (pred == labels).sum() train_acc += train_correct.item() if i == len(train_dataloader) - 1: print(&#39;Training [%d, %5d] loss: %.3f acc: %.3f&#39; % (epoch + 1, i + 1, running_loss / len(train_dataset), train_acc / len(train_dataset))) # 更新窗口图像 viz.line([[running_loss / len(train_dataset), train_acc / len(train_dataset)]], [epoch], win=&#39;train&#39;, update=&#39;append&#39;) running_loss = 0.0 # 验证集 net.eval() eval_loss = 0.0 eval_acc = 0.0 for i, data in enumerate(test_dataloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) outputs = net(inputs) loss = criterion(outputs, labels) eval_loss += loss.item() _, pred = torch.max(outputs.data, 1) num_correct = (pred == labels).sum() eval_acc += num_correct.item() if i == len(test_dataloader) - 1: print(&#39;Evaluating [%d, %5d] loss: %.3f acc: %.3f&#39; % (epoch + 1, i + 1, eval_loss / len(test_dataset), eval_acc / len(test_dataset))) viz.line([[eval_loss / len(test_dataset), eval_acc / len(test_dataset)]], [epoch], win=&#39;test&#39;, update=&#39;append&#39;) eval_loss = 0.0 torch.save(net.state_dict(), &#39;model/logos_v2.pkl&#39;) print(&#39;Finished Training&#39;) # 预测 dataiter = iter(test_dataloader) images, labels = dataiter.next() #imshow(torchvision.utils.make_grid(images)) print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4))) images, labels = images.to(device), labels.to(device) #outputs = net(images) net.load_state_dict(torch.load(&#39;model/logos_v2.pkl&#39;)) outputs = net(images) _, predicted = torch.max(outputs, 1) print(&#39;Predicted: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[predicted[j]] for j in range(4))) # 整个数据集预测 correct = 0 total = 0 with torch.no_grad(): for data in test_dataloader: images, labels = data images, labels = images.to(device), labels.to(device) #outputs = net(images) net.load_state_dict(torch.load(&#39;model/logos_v2.pkl&#39;)) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total))","categories":[{"name":"图像分类","slug":"图像分类","permalink":"http://xiaohongzhong.top/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://xiaohongzhong.top/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://xiaohongzhong.top/tags/Pytorch/"},{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"深度学习之计算机视觉概念","slug":"深度学习之计算机视觉概念","date":"2020-12-08T03:06:19.000Z","updated":"2020-12-08T04:15:39.017Z","comments":true,"path":"2020/12/08/shen-du-xue-xi-zhi-ji-suan-ji-shi-jue-gai-nian/","link":"","permalink":"http://xiaohongzhong.top/2020/12/08/shen-du-xue-xi-zhi-ji-suan-ji-shi-jue-gai-nian/","excerpt":"","text":"卷积填充 池化它们会降低特征平面和卷积层输出的大小池化提供两种不同的功能：一个是减少要处理的数据大小，一个是强制算法不关注图像位置的微小变化 激活函数ReLU","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"图像分类-CIFAR10-Pytorch","slug":"图像分类-CIFAR10-Pytorch","date":"2020-12-04T02:13:46.000Z","updated":"2020-12-08T04:13:43.393Z","comments":true,"path":"2020/12/04/tu-xiang-fen-lei-cifar10-pytorch/","link":"","permalink":"http://xiaohongzhong.top/2020/12/04/tu-xiang-fen-lei-cifar10-pytorch/","excerpt":"","text":"想要使用pytorch，就从最基础的图像分类，也是最熟悉的图像分类开始。 这是官网中文中的代码 http://pytorch123.com/SecondSection/training_a_classifier/ 数据集如果直接从源代码下载CIFAR10数据集，本来就只有160M，结果下载一天也没下载好，下载太慢。建议从网上寻找数据集。 下载完之后，需要修改源码内容。打开我在anaconda下安装torch的虚拟环境，找到torchvision的包，在datasets文件夹下面有个cifar.py C:\\Users\\DELL\\Anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\datasets\\cifar.py class CIFAR10(VisionDataset): &quot;&quot;&quot;`CIFAR10 &lt;https://www.cs.toronto.edu/~kriz/cifar.html&gt;`_ Dataset. Args: root (string): Root directory of dataset where directory ``cifar-10-batches-py`` exists or will be saved to if download is set to True. train (bool, optional): If True, creates dataset from training set, otherwise creates from test set. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. &quot;&quot;&quot; base_folder = &#39;cifar-10-batches-py&#39; #url = &quot;https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz&quot; url = &quot;file:///D:/work/search_image/torch/dataset/cifar-10-python.tar.gz&quot; filename = &quot;cifar-10-python.tar.gz&quot; tgz_md5 = &#39;c58f30108f718f92721af3b95e74349a&#39; train_list = [ [&#39;data_batch_1&#39;, &#39;c99cafc152244af753f735de768cd75f&#39;], [&#39;data_batch_2&#39;, &#39;d4bba439e000b95fd0a9bffe97cbabec&#39;], [&#39;data_batch_3&#39;, &#39;54ebc095f3ab1f0389bbae665268c751&#39;], [&#39;data_batch_4&#39;, &#39;634d18415352ddfa80567beed471001a&#39;], [&#39;data_batch_5&#39;, &#39;482c414d41f54cd18b22e5b47cb7c3cb&#39;], ] 修改url，把url变成file:///D:/work/search_image/torch/dataset/cifar-10-python.tar.gz（这个是我下载的cifar10放置的位置）之后运行 import torchvision as tv import torchvision.transforms as transforms from torchvision.transforms import ToPILImage import torch.nn as nn import torch.nn.functional as F import torch show = ToPILImage() # 可以把tensor转成Image，方便可视化 transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) trainset = tv.datasets.CIFAR10(root=&#39;D:/work/search_image/torch/dataset/&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = tv.datasets.CIFAR10(root=&#39;D:/work/search_image/torch/dataset/&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) (data, label) = trainset[100] print(classes[label]) print(len(trainset)) print(len(testset)) 输出： Files already downloaded and verified Files already downloaded and verified ship 50000 10000 创建网络class Net_my(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(x.size()[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 本来报错： class Net_my(nn.modules):TypeError: module.init() takes at most 2 arguments (3 given) nn.modules -&gt; nn.Module就ok了。 训练报错 BrokenPipeError: [Errno 32] Broken pipe 好像是win10的多线程导致的，需要避免windows使用多线程。把torch.utils.data.DataLoader()函数时的 num_workers 参数改成0。 RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same 要么都是cpu，要么都是gpu，需要统一 net.cuda()device = torch.device(“cuda:0” if torch.cuda.is_available() else “cpu”)#网络和输入的数据都需要转成gpu或者cpu。 可以训练 # net = Net() # net.cuda() # from torch import optim # criterion = nn.CrossEntropyLoss() # optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # # torch.set_num_threads(8) # for epoch in range(10): # running_loss = 0.0 # total = 0 # correct = 0 # for i, data in enumerate(trainloader, 0): # inputs, labels = data # inputs, labels = inputs.to(device),labels.to(device) # # optimizer.zero_grad() # outputs = net(inputs) # loss = criterion(outputs, labels) # loss.backward() # # optimizer.step() # # running_loss += loss.item() # # if i % 2000 == 1999: # print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch+1, i+1, running_loss / 2000)) # running_loss = 0.0 # _, predicted = torch.max(outputs, 1) # total += labels.size(0) # correct += (predicted == labels).sum().item() # print(&#39;accuracy of the network on the %d train images: %.3f %%&#39; % (total, 100.0 * correct / total)) # total = 0 # correct = 0 # # print(&#39;Finished Training&#39;) 使用预训练模型来图像分类import torchvision as tv import torchvision.transforms as transforms from torchvision.transforms import ToPILImage import torch.nn as nn import torch.nn.functional as F import torch from torchvision import models from PIL import Image alexnet = models.alexnet(pretrained=True) print(alexnet) img = Image.open(&#39;1.jpg&#39;) transform = transforms.Compose([ #[1] transforms.Resize(256), #[2] transforms.CenterCrop(224), #[3] transforms.ToTensor(), #[4] transforms.Normalize( #[5] mean=[0.485, 0.456, 0.406], #[6] std=[0.229, 0.224, 0.225] #[7] )]) img_t = transform(img) batch_t = torch.unsqueeze(img_t, 0) alexnet.eval() out = alexnet(batch_t) with open(&#39;imagenet_classes.txt&#39;) as f: classes = [line.strip() for line in f.readlines()] _, indices = torch.sort(out, descending=True) percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100 [print(classes[idx], percentage[idx].item()) for idx in indices[0][:5]] 输出 655: &#39;miniskirt, mini&#39;, 10.492936134338379 765: &#39;rocking chair, rocker&#39;, 4.194277286529541 545: &#39;electric fan, blower&#39;, 4.150004863739014 411: &#39;apron&#39;, 2.946284532546997 589: &#39;hand blower, blow dryer, blow drier, hair dryer, hair drier&#39;, 2.749168872833252 完整代码import torch import torchvision import torchvision.transforms as transforms device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 使用torchvision加载并且归一化CIFAR10的训练和测试数据集 transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=&#39;D:/work/search_image/torch/dataset/&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0) testset = torchvision.datasets.CIFAR10(root=&#39;D:/work/search_image/torch/dataset/&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=0) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) # 显示一批图像 import matplotlib.pyplot as plt import numpy as np def imshow(img): img = img / 2 + 0.5 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() dataiter = iter(trainloader) images, labels = dataiter.next() #imshow(torchvision.utils.make_grid(images)) print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4))) # 定义一个卷积神经网络 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() net.to(device) # 定义一个损失函数和优化器 让我们使用分类交叉熵Cross-Entropy 作损失函数，动量SGD做优化器。 import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 训练网络 for epoch in range(5): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 1562 == 1555: print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 torch.save(net.state_dict(), &#39;model/cifar10.pkl&#39;) print(&#39;Finished Training&#39;) # 预测 dataiter = iter(testloader) images, labels = dataiter.next() #imshow(torchvision.utils.make_grid(images)) print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4))) images, labels = images.to(device), labels.to(device) #outputs = net(images) net.load_state_dict(torch.load(&#39;model/cifar10.pkl&#39;)) outputs = net(images) _, predicted = torch.max(outputs, 1) print(&#39;Predicted: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[predicted[j]] for j in range(4))) # 整个数据集预测 correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data images, labels = images.to(device), labels.to(device) #outputs = net(images) net.load_state_dict(torch.load(&#39;model/cifar10.pkl&#39;)) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total)) # 最后测试准确率在53%左右","categories":[{"name":"图像分类","slug":"图像分类","permalink":"http://xiaohongzhong.top/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://xiaohongzhong.top/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://xiaohongzhong.top/tags/Pytorch/"},{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"跑通图像压缩算法SReC","slug":"SReC","date":"2020-12-03T06:41:52.000Z","updated":"2020-12-09T09:11:49.959Z","comments":true,"path":"2020/12/03/srec/","link":"","permalink":"http://xiaohongzhong.top/2020/12/03/srec/","excerpt":"","text":"环境：win10 源码位置：https://github.com/caoscott/SReC 下载源码git clone &#103;&#105;&#116;&#64;&#x67;&#x69;&#116;&#x68;&#117;&#x62;&#46;&#99;&#111;&#x6d;:caoscott/SReC.git 搭建环境，使用的是没有nvidia-docker的方法我是已经装了torch1.5.0 + torchvision0.6.0 + gpu的环境，接下来 pip install -r requirements.txt 安装gcc，参考的是这个大佬的文章https://blog.csdn.net/qilimi1053620912/article/details/88573017在windows10上安装gcc。 运行COMPILE_CUDA=force python3 setup.py install，报错 error: command ‘C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX86\\x64\\cl.exe’ failed with exit status 2 搞不定啊","categories":[{"name":"跑过的算法","slug":"跑过的算法","permalink":"http://xiaohongzhong.top/categories/%E8%B7%91%E8%BF%87%E7%9A%84%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"图像压缩","slug":"图像压缩","permalink":"http://xiaohongzhong.top/tags/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/"},{"name":"图像处理","slug":"图像处理","permalink":"http://xiaohongzhong.top/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"supervisor + scrapy + python","slug":"supervisor-scrapy-python","date":"2020-11-26T06:42:57.000Z","updated":"2020-12-08T04:15:20.963Z","comments":true,"path":"2020/11/26/supervisor-scrapy-python/","link":"","permalink":"http://xiaohongzhong.top/2020/11/26/supervisor-scrapy-python/","excerpt":"","text":"在ubuntu16.04下，首先爬取新浪微博的话题和关键词，然后会返回爬到的微博用户id，bid，和图像url。之后再把这些图像送到深度学习所做的图像侵权检测系统，判断是否侵权我们的素材，返回相关的信息。 1.图像侵权检测 2.python 开启http服务 3.scrapy 爬取新浪微博 4.supervisor 进程管理 其实123慢慢调好了，这周调的最多的是怎么把1234放在一起，碰到了相当多的问题。 开启supervisor 执行的命令可以是虚拟环境的命令，需要加上路径比如 [program:ibaotu-image-match]command=/usr/local/data/anaconda3/envs/snakes/bin/python /usr/local/data/www/weibo_search_master/http_server_GET.pydirectory=/usr/local/data/www/weibo_search_masterautostart=trueautorestart=truestdout_logfile=/usr/local/data/www/log/out.logstderr_logfile=/usr/local/data/www/log/err.log 执行命令的时候就知道是执行哪里的python了 还可以web查看管理supervisor [inet_http_server] ; inet (TCP) server disabled by defaultport=*:9001 ; (ip_address:port specifier, *:port for all iface);username=admin ; (default is no username (open server));password=admin ; (default is no password (open server)) supervisor命令查看状态supervisorctl status更新supervisorctl update重置supervisorctl reload结束supervisorctl stop all开始supervisorctl start all更新配置supervisord -c /etc/supervisord/supervisord.conf 遇到的报错 unix:///var/run/supervisor.sock no such file 1.sudo touch /var/run/supervisor.sock 2.sudo chmod 777 /var/run/supervisor.sock 3.sudo service supervisor restart unix:///var/run/supervisor/supervisor.sock refused connection supervisord -c /etc/supervisord/supervisord.conf启动supervisord并使用配置 The ‘supervisor==3.2.0’ distribution was not found and is required by the application 如果默认的python是python2,应该不会报错。如果是python3，就要修改和terminator一样也是python版本引起的，编辑/usr/bin/supervisord将#!/usr/bin/python修改为#!/usr/bin/python2即可貌似启动supervisor 只能用python2 supervisord不能正常地话，查看它的log supervisorctl tail ibaotu-image-match stderr supervisorctl 启动起来，一直在报错重启， OSError: [Errno 98] Address already in use 1.netstat -tunlpkill -9 6153这个是每次都要这样，有点烦2.find / -name supervisor.sockunlink /name/supervisor.sock root@ibaotu-algo:/usr/bin# scrapyTraceback (most recent call last): File “/usr/local/bin/scrapy”, line 7, in from scrapy.cmdline import execute File “/usr/local/lib/python3.5/dist-packages/scrapy/init.py”, line 12, in from scrapy.spiders import Spider File “/usr/local/lib/python3.5/dist-packages/scrapy/spiders/init.py”, line 22 name: Optional[str] = None ^SyntaxError: invalid syntax 在虚拟环境是好的，但是放在一起跑就出问题了。我研究了好久，才发现还是命令路径的问题。在终端输入scrapy，虚拟环境是ok的，在普通的就报错。两个scrapy版本是一样的，只不过虚拟环境的python是3.6.5，普通的python是3.5.2，不知道是不是和这个有关。把scrapy改成/usr/local/data/anaconda3/envs/snakes/bin/scrapy scrapy是命令行，怎么在代码里面添加？ #cmdline.execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;search&quot;, &quot;-a&quot;, &quot;start_date=%s&quot;%(start_date), &quot;-a&quot;, &quot;end_date=%s&quot;%(end_date), &quot;-a&quot;, &quot;keyword_list=%s&quot;%(keyword)]) cmd = &quot;/usr/local/data/anaconda3/envs/snakes/bin/scrapy crawl search -a &quot; + &quot;start_date=%s&quot;%(start_date) + &quot; -a &quot; + &quot;end_date=%s&quot;%(end_date) + &quot; -a &quot; + &quot;keyword_list=%s&quot;%(keyword) os.system(cmd) 开启的http服务，客户请求爬虫，怎么把数据返回？ 一开始的想法是能不能搞个全局变量。怎么搞也搞不过去，python跨文件的全局变量，后面知道scrapy是新开的进程，数据根本到不了。后面把数据存到csv或者json文件，爬完再读取，再返回。 no module named … 因为路径的问题，也搞了好久，明明都添加了，还是报错，sys.path.append()最后因为没有使用到全局变量，这个报错也就不了了之了。 遍历字典，写入csv，一个数据固定的写入一行 writer.writerow([item[‘weibo’][key] for key in item[‘weibo’].keys() if key == ‘id’ or key == ‘bid’ or key == ‘pics’]) 是csv转成json，还是直接爬取pipeline到json 我选择了后者，爬取了json格式的文件下来，包括id，bid，picurl组成的json格式的数据。一开始json格式还写不进去，应该是格式问题，我弄了data格式，之后ok了。 with codecs.open(jsonfile_path, &#39;a+&#39;) as f: data = &#123; &#39;id&#39;: item[&#39;weibo&#39;][&#39;user_id&#39;], &#39;bid&#39;: item[&#39;weibo&#39;][&#39;bid&#39;], &#39;screen_name&#39;: item[&#39;weibo&#39;][&#39;screen_name&#39;], &#39;pics&#39;: item[&#39;weibo&#39;][&#39;pics&#39;] &#125; lines = json.dumps(data, ensure_ascii=False) f.write(lines + &quot;\\n&quot;) 然后也出现supervisor一直重启的问题 因为配置中restart:默认为true，然后一直有htpp进程开着，报错OSError: [Errno 98] Address already in use；还有个原因就是原来的程序不是死循环，一会运行结束，又重启了。 最后这个1234基本就能愉快地在一起玩耍了。","categories":[{"name":"项目经验","slug":"项目经验","permalink":"http://xiaohongzhong.top/categories/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaohongzhong.top/tags/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","permalink":"http://xiaohongzhong.top/tags/python/"}]},{"title":"Conda的使用","slug":"Conda的使用","date":"2020-11-23T13:00:20.000Z","updated":"2020-12-08T04:12:26.119Z","comments":true,"path":"2020/11/23/conda-de-shi-yong/","link":"","permalink":"http://xiaohongzhong.top/2020/11/23/conda-de-shi-yong/","excerpt":"","text":"安装Anaconda首先需要安装anaconda，在不同的电脑上下载使用很多次了，还是挺方便的。正好开通了博客，就记录下conda的使用。 conda 创建虚拟环境anaconda安装成功之后，如果不成功，网上很多安装的博客可以查看。应该有conda命令了。 conda -V 返回conda 4.7.12 1.创建虚拟环境命令：conda create --name pytorch python=3.6.5 安装总是失败 Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): ...working... 我添加了conda换源，可是还是不行 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes 修改命令conda create --name pytorch python=3.6 $ conda create -n tensorflow python=3.6.5 Collecting package metadata (repodata.json): done Solving environment: done ==&gt; WARNING: A newer version of conda exists. &lt;== current version: 4.8.3 latest version: 4.9.2 Please update conda by running $ conda update -n base -c defaults conda ## Package Plan ## environment location: C:\\Users\\DELL\\Anaconda3\\envs\\tensorflow added / updated specs: - python=3.6.5 The following packages will be downloaded: package | build ---------------------------|----------------- certifi-2020.11.8 | py36haa95532_0 151 KB pip-20.2.4 | py36haa95532_0 2.1 MB python-3.6.5 | h0c2934d_0 21.6 MB setuptools-50.3.1 | py36haa95532_1 939 KB vc-14.1 | h0510ff6_4 6 KB vs2015_runtime-14.16.27012 | hf0eaf9b_3 2.4 MB wincertstore-0.2 | py36h7fe50ca_0 13 KB ------------------------------------------------------------ Total: 27.2 MB The following NEW packages will be INSTALLED: done # # To activate this environment, use # # $ conda activate tensorflow # # To deactivate an active environment, use # # $ conda deactivate 2.安装成功之后，可以查看确认下conda env list conda 进入虚拟环境conda activate tensorflow或者source activate tensorflow conda 退出虚拟环境conda deactivate或者试试source deactivate没有退出，就多输入几次命令 conda 添加镜像conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/#显示安装的镜像conda config –set show_channel_urls yes#已添加的channel在哪里查看.condarc conda 删除虚拟环境conda remove -n tensorflow –all conda remove -n tensorflow --all Remove all packages in environment C:\\Users\\DELL\\Anaconda3\\envs\\tensorflow: ## Package Plan ## environment location: C:\\Users\\DELL\\Anaconda3\\envs\\tensorflow The following packages will be REMOVED: certifi-2020.11.8-py36haa95532_0 pip-20.2.4-py36haa95532_0 python-3.6.5-h0c2934d_0 setuptools-50.3.1-py36haa95532_1 vc-14.1-h0510ff6_4 vs2015_runtime-14.16.27012-hf0eaf9b_3 wheel-0.35.1-pyhd3eb1b0_0 wincertstore-0.2-py36h7fe50ca_0 Proceed ([y]/n)? y Preparing transaction: done Verifying transaction: done Executing transaction: done 我正常用的就是这些命令，记录一下。","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://xiaohongzhong.top/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"anaconda","slug":"anaconda","permalink":"http://xiaohongzhong.top/tags/anaconda/"}]},{"title":"开发工具","slug":"开发工具","date":"2020-11-19T06:28:49.000Z","updated":"2020-12-08T04:13:10.214Z","comments":true,"path":"2020/11/19/kai-fa-gong-ju/","link":"","permalink":"http://xiaohongzhong.top/2020/11/19/kai-fa-gong-ju/","excerpt":"","text":"这里是我日常使用的一些工具，主要是Windows上的，也有Ubuntu的。编辑器：Atom （使用体验超级好）Packages：1.Hydropen2.activate_power_mode3.atom_python_run4.atom_terminal5.autocomplete-python6.hyperclick7.ide-python8.kite # 特别好用，使用有惊喜9.minimap10.python-tools11.run-in-terminal其他很多包全是core packages，不进行列举。 远程连接传输工具：Xshell Winscp PuttyCoding：Pycharm Anaconda Visual-Studio Source-Insight数据库：HeidiSQL查看神经网络的结构：NetronCuda：v10.0Cudnn：v7.6.5查看windows文件：listary","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://xiaohongzhong.top/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://xiaohongzhong.top/tags/%E5%B7%A5%E5%85%B7/"}]}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"图像分类","slug":"图像分类","permalink":"http://xiaohongzhong.top/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"},{"name":"跑过的算法","slug":"跑过的算法","permalink":"http://xiaohongzhong.top/categories/%E8%B7%91%E8%BF%87%E7%9A%84%E7%AE%97%E6%B3%95/"},{"name":"项目经验","slug":"项目经验","permalink":"http://xiaohongzhong.top/categories/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C/"},{"name":"开发工具","slug":"开发工具","permalink":"http://xiaohongzhong.top/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://xiaohongzhong.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"经典神经网络","slug":"经典神经网络","permalink":"http://xiaohongzhong.top/tags/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"图像处理","slug":"图像处理","permalink":"http://xiaohongzhong.top/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://xiaohongzhong.top/tags/Pytorch/"},{"name":"图像压缩","slug":"图像压缩","permalink":"http://xiaohongzhong.top/tags/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaohongzhong.top/tags/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","permalink":"http://xiaohongzhong.top/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"http://xiaohongzhong.top/tags/anaconda/"},{"name":"工具","slug":"工具","permalink":"http://xiaohongzhong.top/tags/%E5%B7%A5%E5%85%B7/"}]}